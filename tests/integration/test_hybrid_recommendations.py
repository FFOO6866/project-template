#!/usr/bin/env python3
"""
Phase 3: Hybrid AI Recommendation Engine Test Script
Comprehensive validation of 4-algorithm hybrid recommendation system

Tests:
1. Hybrid engine initialization
2. Individual algorithm scoring
3. Weighted score fusion
4. Explainability features
5. Caching functionality
6. Performance benchmarks
7. Accuracy comparison (basic vs hybrid)
"""

import os
import sys
import logging
import time
from datetime import datetime
from typing import Dict, List
import json

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Add src to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))


class HybridRecommendationTester:
    """Comprehensive test suite for hybrid recommendation engine"""

    def __init__(self):
        """Initialize tester"""
        self.test_results = {
            'total_tests': 0,
            'passed_tests': 0,
            'failed_tests': 0,
            'test_details': []
        }

    def log_test(self, test_name: str, passed: bool, details: str = ""):
        """Log test result"""
        self.test_results['total_tests'] += 1
        if passed:
            self.test_results['passed_tests'] += 1
            logger.info(f"‚úÖ PASSED: {test_name}")
        else:
            self.test_results['failed_tests'] += 1
            logger.error(f"‚ùå FAILED: {test_name}")

        self.test_results['test_details'].append({
            'test_name': test_name,
            'passed': passed,
            'details': details,
            'timestamp': datetime.now().isoformat()
        })

    def test_1_hybrid_engine_initialization(self) -> bool:
        """Test 1: Hybrid engine initialization"""
        logger.info("\n" + "=" * 80)
        logger.info("TEST 1: Hybrid Engine Initialization")
        logger.info("=" * 80)

        try:
            from src.ai.hybrid_recommendation_engine import HybridRecommendationEngine

            # Initialize engine
            engine = HybridRecommendationEngine()

            # Verify components
            checks = [
                ("Database connection", engine.database is not None),
                ("Knowledge graph connection", engine.knowledge_graph is not None),
                ("OpenAI client", engine.openai_client is not None),
                ("Embedding model", engine.embedding_model is not None),
                ("Algorithm weights", sum(engine.weights.values()) == 1.0),
            ]

            all_passed = True
            for check_name, result in checks:
                status = "‚úÖ" if result else "‚ùå"
                logger.info(f"  {status} {check_name}: {result}")
                if not result:
                    all_passed = False

            self.log_test("Hybrid engine initialization", all_passed)
            return all_passed

        except Exception as e:
            logger.error(f"Initialization test failed: {e}")
            self.log_test("Hybrid engine initialization", False, str(e))
            return False

    def test_2_individual_algorithm_scoring(self) -> bool:
        """Test 2: Individual algorithm scoring"""
        logger.info("\n" + "=" * 80)
        logger.info("TEST 2: Individual Algorithm Scoring")
        logger.info("=" * 80)

        try:
            from src.ai.hybrid_recommendation_engine import HybridRecommendationEngine

            engine = HybridRecommendationEngine()

            # Sample product
            product = {
                'id': 1,
                'name': 'Industrial LED Light 50W',
                'description': 'High-efficiency LED lighting for industrial use',
                'category': 'Lighting',
                'brand': 'TestBrand',
                'price': 89.99
            }

            # Sample requirements
            requirements = ['LED light', 'industrial lighting', '50W power']

            # Test each algorithm
            algorithms = [
                ("Collaborative filtering", lambda: engine._collaborative_score(product, None)),
                ("Content-based filtering", lambda: engine._content_based_score(product, "industrial LED light", requirements)),
                ("Knowledge graph", lambda: engine._knowledge_graph_score(product, requirements)),
                ("LLM analysis", lambda: engine._llm_analysis_score(product, requirements))
            ]

            all_passed = True
            scores = {}

            for algo_name, score_func in algorithms:
                try:
                    score = score_func()
                    scores[algo_name] = score

                    # Verify score is in valid range [0, 1]
                    valid = 0.0 <= score <= 1.0

                    status = "‚úÖ" if valid else "‚ùå"
                    logger.info(f"  {status} {algo_name}: {score:.3f}")

                    if not valid:
                        all_passed = False

                except Exception as e:
                    logger.error(f"  ‚ùå {algo_name}: Failed - {e}")
                    all_passed = False

            self.log_test("Individual algorithm scoring", all_passed, json.dumps(scores, indent=2))
            return all_passed

        except Exception as e:
            logger.error(f"Algorithm scoring test failed: {e}")
            self.log_test("Individual algorithm scoring", False, str(e))
            return False

    def test_3_weighted_score_fusion(self) -> bool:
        """Test 3: Weighted score fusion"""
        logger.info("\n" + "=" * 80)
        logger.info("TEST 3: Weighted Score Fusion")
        logger.info("=" * 80)

        try:
            from src.ai.hybrid_recommendation_engine import HybridRecommendationEngine

            engine = HybridRecommendationEngine()

            # Sample RFP
            rfp_text = """
            We need industrial LED lighting for our warehouse.
            Requirements:
            - 20 units of 50W LED lights
            - High efficiency and durability
            - Suitable for 24/7 operation
            """

            # Get recommendations
            recommendations = engine.recommend_products(
                rfp_text=rfp_text,
                limit=5,
                explain=True
            )

            # Verify recommendations
            checks = [
                ("Recommendations returned", len(recommendations) > 0),
                ("Hybrid scores present", all('hybrid_score' in rec for rec in recommendations)),
                ("Scores in valid range", all(0 <= rec['hybrid_score'] <= 1 for rec in recommendations)),
                ("Scores sorted descending", all(recommendations[i]['hybrid_score'] >= recommendations[i+1]['hybrid_score'] for i in range(len(recommendations)-1))),
                ("Algorithm scores present", all('algorithm_scores' in rec for rec in recommendations))
            ]

            all_passed = True
            for check_name, result in checks:
                status = "‚úÖ" if result else "‚ùå"
                logger.info(f"  {status} {check_name}: {result}")
                if not result:
                    all_passed = False

            # Log sample recommendation
            if recommendations:
                top_rec = recommendations[0]
                logger.info(f"\n  Top recommendation:")
                logger.info(f"    Product: {top_rec['product'].get('name')}")
                logger.info(f"    Hybrid score: {top_rec['hybrid_score']:.3f}")
                logger.info(f"    Algorithm scores: {json.dumps(top_rec['algorithm_scores'], indent=6)}")

            self.log_test("Weighted score fusion", all_passed)
            return all_passed

        except Exception as e:
            logger.error(f"Score fusion test failed: {e}")
            self.log_test("Weighted score fusion", False, str(e))
            return False

    def test_4_explainability_features(self) -> bool:
        """Test 4: Explainability features"""
        logger.info("\n" + "=" * 80)
        logger.info("TEST 4: Explainability Features")
        logger.info("=" * 80)

        try:
            from src.ai.hybrid_recommendation_engine import HybridRecommendationEngine

            engine = HybridRecommendationEngine()

            # Sample RFP
            rfp_text = "We need safety equipment for construction work"

            # Get recommendations with explanations
            recommendations = engine.recommend_products(
                rfp_text=rfp_text,
                limit=3,
                explain=True
            )

            # Verify explanations
            checks = [
                ("Explanations present", all('explanation' in rec for rec in recommendations)),
                ("Top reasons provided", all(rec['explanation'].get('top_reasons') for rec in recommendations if rec.get('explanation'))),
                ("Product names in explanations", all(rec['explanation'].get('product_name') for rec in recommendations if rec.get('explanation')))
            ]

            all_passed = True
            for check_name, result in checks:
                status = "‚úÖ" if result else "‚ùå"
                logger.info(f"  {status} {check_name}: {result}")
                if not result:
                    all_passed = False

            # Log sample explanation
            if recommendations and recommendations[0].get('explanation'):
                explanation = recommendations[0]['explanation']
                logger.info(f"\n  Sample explanation:")
                logger.info(f"    Product: {explanation.get('product_name')}")
                logger.info(f"    Reasons:")
                for reason in explanation.get('top_reasons', []):
                    logger.info(f"      - {reason}")

            self.log_test("Explainability features", all_passed)
            return all_passed

        except Exception as e:
            logger.error(f"Explainability test failed: {e}")
            self.log_test("Explainability features", False, str(e))
            return False

    def test_5_caching_functionality(self) -> bool:
        """Test 5: Caching functionality"""
        logger.info("\n" + "=" * 80)
        logger.info("TEST 5: Caching Functionality")
        logger.info("=" * 80)

        try:
            from src.ai.hybrid_recommendation_engine import HybridRecommendationEngine

            engine = HybridRecommendationEngine()

            if not engine.cache_enabled:
                logger.warning("  ‚ö†Ô∏è Redis cache not available, skipping cache tests")
                self.log_test("Caching functionality", True, "Skipped - cache not available")
                return True

            # Sample RFP
            rfp_text = "Need LED lights for office"

            # First request (cache miss)
            start_time = time.time()
            recommendations1 = engine.recommend_products(rfp_text=rfp_text, limit=5)
            time1 = time.time() - start_time

            # Second request (cache hit)
            start_time = time.time()
            recommendations2 = engine.recommend_products(rfp_text=rfp_text, limit=5)
            time2 = time.time() - start_time

            # Verify caching
            checks = [
                ("Cached results returned", len(recommendations2) > 0),
                ("Results consistent", len(recommendations1) == len(recommendations2)),
                ("Cache faster than first run", time2 < time1 or time2 < 0.1)  # Cache should be significantly faster
            ]

            all_passed = True
            for check_name, result in checks:
                status = "‚úÖ" if result else "‚ùå"
                logger.info(f"  {status} {check_name}: {result}")
                if not result:
                    all_passed = False

            logger.info(f"\n  Performance:")
            logger.info(f"    First request: {time1*1000:.1f}ms")
            logger.info(f"    Cached request: {time2*1000:.1f}ms")
            logger.info(f"    Speedup: {time1/time2:.1f}x" if time2 > 0 else "    Speedup: N/A")

            # Clear cache
            engine.clear_cache()
            logger.info(f"  ‚úÖ Cache cleared successfully")

            self.log_test("Caching functionality", all_passed)
            return all_passed

        except Exception as e:
            logger.error(f"Caching test failed: {e}")
            self.log_test("Caching functionality", False, str(e))
            return False

    def test_6_performance_benchmarks(self) -> bool:
        """Test 6: Performance benchmarks"""
        logger.info("\n" + "=" * 80)
        logger.info("TEST 6: Performance Benchmarks")
        logger.info("=" * 80)

        try:
            from src.ai.hybrid_recommendation_engine import HybridRecommendationEngine

            engine = HybridRecommendationEngine()

            # Test cases with different complexities
            test_cases = [
                ("Short RFP", "LED lights", 5),
                ("Medium RFP", "We need industrial lighting equipment including LED lights and motion sensors", 10),
                ("Long RFP", """
                    Request for Proposal - Warehouse Lighting Upgrade

                    We are seeking a comprehensive lighting solution for our 50,000 sq ft warehouse.
                    Requirements include:
                    - 100 high-bay LED fixtures (150W each)
                    - Motion sensors for energy savings
                    - Emergency lighting system
                    - Dimming controls
                    - 10-year warranty

                    Please provide detailed specifications and pricing.
                """, 20)
            ]

            all_passed = True
            performance_results = []

            for test_name, rfp_text, limit in test_cases:
                try:
                    start_time = time.time()
                    recommendations = engine.recommend_products(rfp_text=rfp_text, limit=limit)
                    elapsed_time = time.time() - start_time

                    # Performance criteria: < 5 seconds for any query
                    meets_criteria = elapsed_time < 5.0

                    status = "‚úÖ" if meets_criteria else "‚ùå"
                    logger.info(f"  {status} {test_name}: {elapsed_time*1000:.1f}ms ({len(recommendations)} results)")

                    performance_results.append({
                        'test': test_name,
                        'time_ms': elapsed_time * 1000,
                        'results': len(recommendations),
                        'meets_criteria': meets_criteria
                    })

                    if not meets_criteria:
                        all_passed = False

                except Exception as e:
                    logger.error(f"  ‚ùå {test_name}: Failed - {e}")
                    all_passed = False

            self.log_test("Performance benchmarks", all_passed, json.dumps(performance_results, indent=2))
            return all_passed

        except Exception as e:
            logger.error(f"Performance test failed: {e}")
            self.log_test("Performance benchmarks", False, str(e))
            return False

    def test_7_accuracy_comparison(self) -> bool:
        """Test 7: Accuracy comparison (basic vs hybrid)"""
        logger.info("\n" + "=" * 80)
        logger.info("TEST 7: Accuracy Comparison (Basic vs Hybrid)")
        logger.info("=" * 80)

        try:
            from src.simplified_horme_system import SimplifiedRFPProcessor

            # Test RFP
            test_rfp = """
            We need equipment for office renovation:
            - 30 LED ceiling lights
            - 10 motion sensors
            - 5 security cameras
            - Network cabling
            - Power supplies
            """

            # Test with basic search
            processor_basic = SimplifiedRFPProcessor(use_hybrid_engine=False)
            start_time = time.time()
            result_basic = processor_basic.process_rfp_simple(test_rfp, "Test Customer")
            time_basic = time.time() - start_time

            # Test with hybrid engine
            processor_hybrid = SimplifiedRFPProcessor(use_hybrid_engine=True)
            start_time = time.time()
            result_hybrid = processor_hybrid.process_rfp_simple(test_rfp, "Test Customer")
            time_hybrid = time.time() - start_time

            # Compare results
            logger.info(f"\n  Basic Search:")
            logger.info(f"    Products matched: {result_basic.get('products_matched', 0)}")
            logger.info(f"    Time: {time_basic*1000:.1f}ms")
            logger.info(f"    Engine: {result_basic.get('recommendation_engine', 'unknown')}")

            logger.info(f"\n  Hybrid AI:")
            logger.info(f"    Products matched: {result_hybrid.get('products_matched', 0)}")
            logger.info(f"    Time: {time_hybrid*1000:.1f}ms")
            logger.info(f"    Engine: {result_hybrid.get('recommendation_engine', 'unknown')}")
            logger.info(f"    Has explanations: {bool(result_hybrid.get('explanations'))}")

            # Verify improvements
            checks = [
                ("Hybrid engine used", result_hybrid.get('recommendation_engine') == 'hybrid_ai'),
                ("Hybrid found products", result_hybrid.get('products_matched', 0) > 0),
                ("Explanations provided", bool(result_hybrid.get('explanations')))
            ]

            all_passed = True
            for check_name, result in checks:
                status = "‚úÖ" if result else "‚ùå"
                logger.info(f"\n  {status} {check_name}: {result}")
                if not result:
                    all_passed = False

            # Target: 25-40% improvement
            # Note: Actual accuracy measurement requires labeled test data
            logger.info(f"\n  üìä Target Improvement: 25-40% over basic search")
            logger.info(f"     (15% basic ‚Üí 55-60% hybrid accuracy)")
            logger.info(f"     Full accuracy testing requires labeled ground truth data")

            self.log_test("Accuracy comparison", all_passed)
            return all_passed

        except Exception as e:
            logger.error(f"Accuracy comparison test failed: {e}")
            self.log_test("Accuracy comparison", False, str(e))
            return False

    def run_all_tests(self) -> Dict:
        """Run all tests"""
        logger.info("\n" + "=" * 80)
        logger.info("PHASE 3: HYBRID AI RECOMMENDATION ENGINE TEST SUITE")
        logger.info("=" * 80)
        logger.info(f"Start time: {datetime.now().isoformat()}")

        start_time = time.time()

        # Run all tests
        tests = [
            self.test_1_hybrid_engine_initialization,
            self.test_2_individual_algorithm_scoring,
            self.test_3_weighted_score_fusion,
            self.test_4_explainability_features,
            self.test_5_caching_functionality,
            self.test_6_performance_benchmarks,
            self.test_7_accuracy_comparison
        ]

        for test_func in tests:
            try:
                test_func()
            except Exception as e:
                logger.error(f"Test execution error: {e}")

        total_time = time.time() - start_time

        # Summary
        logger.info("\n" + "=" * 80)
        logger.info("TEST SUMMARY")
        logger.info("=" * 80)
        logger.info(f"Total tests: {self.test_results['total_tests']}")
        logger.info(f"‚úÖ Passed: {self.test_results['passed_tests']}")
        logger.info(f"‚ùå Failed: {self.test_results['failed_tests']}")
        logger.info(f"‚è±Ô∏è  Total time: {total_time:.2f}s")
        logger.info(f"End time: {datetime.now().isoformat()}")

        # Calculate pass rate
        if self.test_results['total_tests'] > 0:
            pass_rate = (self.test_results['passed_tests'] / self.test_results['total_tests']) * 100
            logger.info(f"üìä Pass rate: {pass_rate:.1f}%")

            if pass_rate == 100:
                logger.info("\nüéâ ALL TESTS PASSED! Phase 3 Hybrid AI Engine is PRODUCTION READY!")
            elif pass_rate >= 80:
                logger.info("\n‚úÖ Most tests passed. Review failed tests before production deployment.")
            else:
                logger.warning("\n‚ö†Ô∏è Multiple tests failed. Address issues before deployment.")

        return self.test_results

    def save_results(self, filename: str = "hybrid_recommendation_test_results.json"):
        """Save test results to JSON file"""
        try:
            with open(filename, 'w') as f:
                json.dump(self.test_results, f, indent=2)
            logger.info(f"\nüíæ Test results saved to: {filename}")
        except Exception as e:
            logger.error(f"Failed to save test results: {e}")


def main():
    """Main test execution"""
    tester = HybridRecommendationTester()

    try:
        # Run all tests
        results = tester.run_all_tests()

        # Save results
        tester.save_results()

        # Exit code based on pass/fail
        if results['failed_tests'] == 0:
            sys.exit(0)
        else:
            sys.exit(1)

    except Exception as e:
        logger.error(f"Test suite execution failed: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
